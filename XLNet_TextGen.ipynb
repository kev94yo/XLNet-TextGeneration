{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Build a bidirectional text generator with XLNet  \n",
    "In this notebook, we will\n",
    "- Learn how to predict masked tokens with pretrained XLNet model, tokenizer in Huggingface\n",
    "- Bi-directionally generate text from original text by predicting masks at both sides  EX) '<mask\\> just ate five <mask\\>' -> 'I just ate five cookies'\n",
    "- Use beam based algorithm to enhance generation qualities of right-to-left prediction\n",
    "\n",
    "original source (https://towardsdatascience.com/build-a-bidirectional-text-generator-with-xlnet-49d9d37b48a9)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Background Information\n",
    "\n",
    "- GPT-2, GPT-3 are great with text generation, but unidirectional (left-to-right)\n",
    "- XLNet is able generate text in both directions by predicting mask tokens\n",
    "\n",
    "## XLNet\n",
    "- Paper by Google Brain and CMU (https://arxiv.org/pdf/1906.08237.pdf)\n",
    "- Easy explanation (https://towardsdatascience.com/xlnet-explained-in-simple-terms-255b9fb2c97c)\n",
    "- Fixes BERT's main problems\n",
    "    1. Corrupts input with <mask> during pretrain, which doesn't appear in fine-tuning\n",
    "    2. Neglects the dependency between masked positions  \n",
    "        ex) Input: \"New York is a city\", masked input: (mask) (mask) is a city\"  \n",
    "            BERT tries to maximize log p(New | is a city) + log p (York | is a city)\n",
    "            No dependency between \"New\" and \"York\", so could result in weird prediction like \"New Francisco is a city\"\n",
    "\n",
    "- Outperforms BERT in 20 different tasks\n",
    "\n",
    "\n",
    "### Main Idea\n",
    "- Integrates the idea of autoregressive models (ex. GPT) and bi-directional context modeling (ex. BERT)\n",
    "\n",
    "- Generalized **autoregressive** model, where next token is dependent on all previous tokens!\n",
    "\n",
    "- Permutation Language Modeling (PLM): Uses all permutations of the input sequence factorization\n",
    "    - Maximize expected log likelihood over all possible permutation of the sequence\n",
    "    \n",
    "    - From permutations, each position learns to utilize contextual information from all positions  \n",
    "\n",
    "    - No mask needed, just need to ignore words that appear later than the target word (Just like Transformer decoder)  \n",
    "    \n",
    "    - **Therefore captures bidirectional context without masks!**\n",
    "    \n",
    "    - EX) Input sentence: \"New(1) York(2) is(3) a(4) city(5)\", and target word is 3rd word \"is\"\n",
    "        - Possible Permutations  \n",
    "        12 **3** 45: P(is | New York)  \n",
    "        2 **3** 145: P(is | York)  \n",
    "        54 **3** 12: P(is | a city) *example of right-to-left context*  \n",
    "        \n",
    "        ![image](https://miro.medium.com/max/1050/1*dMgzP_YboxpR8VXuGeAg_Q.png)\n",
    "\n",
    "    - Captures dependencies, since it is an **autoregressive** model!\n",
    "        - EX) Current permutation \\[is a city New York]  \n",
    "        \n",
    "            XLNet, being an autoregressive model, predicts in the order of the sequence  \n",
    "            \n",
    "            Computes: log p (New | is a city) + log p (York | New, is a city)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1. Install needed modules\n",
    "We only need the transformers library, which provides a simple interface to XLNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "source": [
    "# 2. Example of masked words prediction with XLNet\n",
    "XLNet can predicted several related masked words while taking into account the previous context  \n",
    "Ex)  “<mask\\> have <mask\\> apples in hands” -> predict what should come in masks!\n",
    "\n",
    "Lets load a tokenizer that processes incoming text into digital form  \n",
    "\n",
    "XLNet uses SentencePiece method\n",
    "\n",
    "SentencePiece\n",
    "- Idea: Not all words are seperated by spaces (Chinese for example, words aren't seperated by spaces in a sentence)\n",
    "- Solution\n",
    "    1. Treat input as raw input stream, including spaces\n",
    "    2. Use Byte-Pair Encoding or unigram to construct vocab\n",
    "\n",
    "- More info here (https://huggingface.co/transformers/tokenizer_summary.html)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "First, load XLNet model and tokenizer from transformers library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict mentioned words in a sentence with XLNet\n",
    "\n",
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')"
   ]
  },
  {
   "source": [
    "Add padding text to help XLNet with short texts (proposed by Aman Rusia [link](https://amanrusia.medium.com/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia\n",
    "# in https://github.com/rusiaaman/XLNet-gen#methodology\n",
    "# and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e\n",
    "\n",
    "PADDING_TEXT = \"\"\"In 1991, the remains of Russian Tsar Nicholas II and his family\n",
    "(except for Alexei and Maria) are discovered.\n",
    "The voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the\n",
    "remainder of the story. 1883 Western Siberia,\n",
    "a young Grigori Rasputin is asked by his father and a group of men to perform magic.\n",
    "Rasputin has a vision and denounces one of the men as a horse thief. Although his\n",
    "father initially slaps him for making such an accusation, Rasputin watches as the\n",
    "man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
    "the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous,\n",
    "with people, even a bishop, begging for his blessing. <eod> </s> <eos>\"\"\""
   ]
  },
  {
   "source": [
    "Predict top 5 words for each <mask\\> token. Feed the model with  \n",
    "\n",
    "1. Tokenized text  \n",
    "2. Masked word indexes  \n",
    "3. Permutation masks (needed to disable input tokens to attend to masked tokens)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Turn padding text + input sentence into tokens with the tokenizer, then turn into tensor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "# We show how to setup inputs to predict a next token using a bi-directional context.torch\n",
    "# We will predict masked tokens\n",
    "input_ids = torch.tensor(tokenizer.encode(PADDING_TEXT + \"I gave you three apples. <mask> have <mask> apples in hands\", add_special_tokens=False)).unsqueeze(0)\n"
   ]
  },
  {
   "source": [
    "Let's check out the input for XLNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "length of sentence: 177\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[   67,  2840,    19,    18,  1484,    20,   965, 29077,  8719,  1273,\n",
       "            21,    45,   273,    17,    10, 15048,    28, 27511,    21,  4185,\n",
       "            11,    41,  2444,     9,    32,  1025,    20,  8719,    26,    23,\n",
       "           673,   966,    19, 29077, 20643, 27511, 20822, 20643,    19,    17,\n",
       "          6616, 17511,    18,  8978,    20,    18,   777,     9, 19233,  1527,\n",
       "         17669,    19,    24,   673,    17, 28756,   150, 12943,  4354,   153,\n",
       "            27,   442,    37,    45,   668,    21,    24,   256,    20,   416,\n",
       "            22,  2771,  4901,     9, 12943,  4354,   153,    51,    24,  3004,\n",
       "            21, 28142,    23,    65,    20,    18,   416,    34,    24,  2958,\n",
       "         22947,     9,  1177,    45,   668,  3097, 13768,    23,   103,    28,\n",
       "           441,   148,    48, 20522,    19, 12943,  4354,   153, 12860,    34,\n",
       "            18,   326,    27, 17492,   684,    21,  6709,     9,  8585,   123,\n",
       "           266,    19, 12943,  4354,   153,  6872,    24,  3004,    20,    18,\n",
       "          9225,  2198,    19, 12717,   103,    22,   401,    24,  6348,     9,\n",
       "         12943,  4354,   153,  1068,  2768,  2286,    19,    33,   104,    19,\n",
       "           176,    24,  9313,    19, 20086,    28,    45, 10292,     9,     7,\n",
       "             2,  7739,  6122,    23,  3151,    96,   675,    44,   139, 18907,\n",
       "             9,     6,    47,     6, 18907,    25,   853]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# All words from Padding Text and given text are tokenized\n",
    "print(\"length of sentence:\", len(input_ids[0]))\n",
    "input_ids"
   ]
  },
  {
   "source": [
    "Create perm_mask, which will tell each input token where the masks are"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "targets = [-6, -4] # index for masks\n",
    "\n",
    "perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n"
   ]
  },
  {
   "source": [
    "Let's see what perm_mask looks like"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([177, 177])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# get matrix of input_ids.length * input_ids.length\n",
    "# each row corresponds to which word is mask, and shouldn't attend to!\n",
    "# mask = 1\n",
    "print(perm_mask[0].shape)\n",
    "perm_mask"
   ]
  },
  {
   "source": [
    "change mask index to 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_mask[0, :, targets] = 1.0 # Previous tokens don't see last token"
   ]
  },
  {
   "source": [
    "We can see index for mask is now 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "perm_mask[0][0] # we can see index for masks are now 1s"
   ]
  },
  {
   "source": [
    "Now we create target_mapping, which points out where our targets to predict are"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "\n",
    "target_mapping = torch.zeros((1, len(targets), input_ids.shape[1]), dtype=torch.float)\n",
    "target_mapping"
   ]
  },
  {
   "source": [
    "First tensor points out first mask, second tensor points out second mask"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 1., 0., 0., 0.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "target_mapping[0, 0, targets[0]] = 1.0 # Our first prediction, first <mask>\n",
    "target_mapping[0, 1, targets[1]] = 1.0 # Our second prediction, second <mask\n",
    "target_mapping"
   ]
  },
  {
   "source": [
    "Let's send all the tensors, model to GPU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_tensor = input_ids.to(\"cuda\")\n",
    "target_mapping_tensor = target_mapping.to(\"cuda\")\n",
    "perm_mask_tensor = perm_mask.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda') # if we have a GPU"
   ]
  },
  {
   "source": [
    "no_grad since we're just doing inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[-23.8164, -31.6897, -32.0603,  ..., -28.9935, -29.9855, -25.5707],\n",
       "         [-38.2683, -45.4465, -45.6469,  ..., -43.8869, -42.4743, -44.8687]]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
    "next_token_logits = outputs[0] # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n",
    "next_token_logits # scores for which word should be in <mask>!"
   ]
  },
  {
   "source": [
    "Use top k to find most probable tokens for masks!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predicted word: <mask> 0\nword and logits You -8.967314720153809\nword and logits I -10.730646133422852\nword and logits We -12.72852611541748\nword and logits Now -14.039458274841309\nword and logits They -14.771018028259277\npredicted word: <mask> 1\nword and logits three -23.03700828552246\nword and logits the -24.339698791503906\nword and logits these -25.60515022277832\nword and logits two -25.806488037109375\nword and logits your -25.95541000366211\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(targets)):\n",
    "    predicted_k_indexes = torch.topk(outputs[0][0][j], k = 5)\n",
    "    predicted_logits_list = predicted_k_indexes[0]\n",
    "    predicted_indexes_list = predicted_k_indexes[1]\n",
    "\n",
    "    print(\"predicted word:\",tokenizer.decode(input_ids[0][targets[j]].item()), j)\n",
    "    for i,item  in enumerate(predicted_indexes_list):\n",
    "        the_index = predicted_indexes_list[i].item()\n",
    "        print(\"word and logits\",tokenizer.decode(the_index),predicted_logits_list[i].item())"
   ]
  },
  {
   "source": [
    "You have 3 apples in hands... Sounds good!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3. Top-K bi-directional generation\n",
    "- Create a loop\n",
    "\n",
    "- At each iteration, model predict top-k tokens for left or right\n",
    "\n",
    "- Add random token off topK and repeat\n",
    "\n",
    "- We try to generate text starting from input sentence \"text generation is cool\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select topK tokens from the probability list and\n",
    "# then based on the selected K word distribution\n",
    "def choose_from_top(probs, k=5, sample_size=1):\n",
    "    ind = np.argpartition(probs, -k)[-k:]\n",
    "    top_prob = probs[ind]\n",
    "    # print(tokenizer.decode(ind))\n",
    "    top_prob = top_prob / np.sum(top_prob) # normalize\n",
    "    choice = np.random.choice(k, sample_size, p = top_prob, replace=False)\n",
    "    token_ids = ind[choice]\n",
    "    return token_ids"
   ]
  },
  {
   "source": [
    "Let's select top 10 and loop 20 times (generate 10 words to start and end)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"text generation is cool\"\n",
    "topk = 10\n",
    "n = 20\n",
    "# Lower temperatures make the model more confident in its top choices,\n",
    "# while temperatures greater than 1 decrease confidence\n",
    "temperature = 5\n",
    "\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda') # if we have a GPU"
   ]
  },
  {
   "source": [
    "Tokenize all the input we need"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = tokenizer.encode(sent, add_special_tokens=False)\n",
    "mask_tokens = tokenizer.encode('<mask>', add_special_tokens=False)\n",
    "padding_tokens = tokenizer.encode(PADDING_TEXT, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1758, 2887, 27, 2299]\n[6]\n[67, 2840, 19, 18, 1484, 20, 965, 29077, 8719, 1273, 21, 45, 273, 17, 10, 15048, 28, 27511, 21, 4185, 11, 41, 2444, 9, 32, 1025, 20, 8719, 26, 23, 673, 966, 19, 29077, 20643, 27511, 20822, 20643, 19, 17, 6616, 17511, 18, 8978, 20, 18, 777, 9, 19233, 1527, 17669, 19, 24, 673, 17, 28756, 150, 12943, 4354, 153, 27, 442, 37, 45, 668, 21, 24, 256, 20, 416, 22, 2771, 4901, 9, 12943, 4354, 153, 51, 24, 3004, 21, 28142, 23, 65, 20, 18, 416, 34, 24, 2958, 22947, 9, 1177, 45, 668, 3097, 13768, 23, 103, 28, 441, 148, 48, 20522, 19, 12943, 4354, 153, 12860, 34, 18, 326, 27, 17492, 684, 21, 6709, 9, 8585, 123, 266, 19, 12943, 4354, 153, 6872, 24, 3004, 20, 18, 9225, 2198, 19, 12717, 103, 22, 401, 24, 6348, 9, 12943, 4354, 153, 1068, 2768, 2286, 19, 33, 104, 19, 176, 24, 9313, 19, 20086, 28, 45, 10292, 9, 7, 2, 7739, 6122, 23, 3151]\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokens)\n",
    "print(mask_tokens)\n",
    "print(padding_tokens)"
   ]
  },
  {
   "source": [
    "Loop 20 times and generate text using methods described above!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "left:  The text generation is cool\n",
      "right:  The text generation is cool.\n",
      "left:  .The text generation is cool.\n",
      "right:  .The text generation is cool. And\n",
      "left:  2.The text generation is cool. And\n",
      "right:  2.The text generation is cool. And you\n",
      "left:  ? 2.The text generation is cool. And you\n",
      "right:  ? 2.The text generation is cool. And you do\n",
      "left:  What? 2.The text generation is cool. And you do\n",
      "right:  What? 2.The text generation is cool. And you do see\n",
      "left:  Like What? 2.The text generation is cool. And you do see\n",
      "right:  Like What? 2.The text generation is cool. And you do see it\n",
      "left:  :Like What? 2.The text generation is cool. And you do see it\n",
      "right:  :Like What? 2.The text generation is cool. And you do see it,\n",
      "left:  1:Like What? 2.The text generation is cool. And you do see it,\n",
      "right:  1:Like What? 2.The text generation is cool. And you do see it, but\n",
      "left:  Question 1:Like What? 2.The text generation is cool. And you do see it, but\n",
      "right:  Question 1:Like What? 2.The text generation is cool. And you do see it, but that\n",
      "left:  . Question 1:Like What? 2.The text generation is cool. And you do see it, but that\n",
      "right:  . Question 1:Like What? 2.The text generation is cool. And you do see it, but that’\n"
     ]
    }
   ],
   "source": [
    "for i in range(n):\n",
    "    input = mask_tokens + sent_tokens + mask_tokens\n",
    "    target_id1 = -len(input) # mask in the beginning\n",
    "    target_id2 = -1 # at the end\n",
    "\n",
    "    input_ids = torch.tensor(padding_tokens + input).unsqueeze(0) # We will predict masked token\n",
    "\n",
    "    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
    "    perm_mask[0, :, [target_id1, target_id2]] = 1.0 # set mask index to 1 (don't attend!)\n",
    "\n",
    "    target_mapping = torch.zeros((1, 2, input_ids.shape[1]), dtype=torch.float)\n",
    "    target_mapping[0, 0, target_id1] = 1.0 # Our first prediction\n",
    "    target_mapping[0, 1, target_id2] = 1.0 # Our second prediction\n",
    "\n",
    "    input_ids_tensor = input_ids.to(\"cuda\")\n",
    "    target_mapping_tensor = target_mapping.to(\"cuda\")\n",
    "    perm_mask_tensor = perm_mask.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad(): # no need to learn anything, just inference\n",
    "        outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
    "    \n",
    "    predicted_tokens = []\n",
    "\n",
    "    for j in range(2):\n",
    "        probs = torch.nn.functional.softmax(outputs[0][0][j]/temperature, dim = 0).to('cpu').numpy()\n",
    "        predicted_tokens.append(choose_from_top(probs, k=topk, sample_size=1))\n",
    "    \n",
    "    if i % 2 == 0: # add to left if iteration number is even\n",
    "        tok = predicted_tokens[0][0]\n",
    "        sent_tokens = [tok] + sent_tokens\n",
    "        print('left: ', tokenizer.decode(sent_tokens))\n",
    "    else: # add to right if iteration number is odd\n",
    "        tok = predicted_tokens[1][0]\n",
    "        sent_tokens = sent_tokens + [tok]\n",
    "        print(\"right: \", tokenizer.decode(sent_tokens))"
   ]
  },
  {
   "source": [
    "Not too impressive..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 4. Top-K-beam bi-directional text generation\n",
    "Difficult for the model to generate text right-to-left  \n",
    "Let's increase the chance of finding connected word sequences by creating certain number of beams, and choosing probable beam  \n",
    "This example uses beam size 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "![image](https://miro.medium.com/max/1500/1*F8pqfzFBsyZwPxWIlhyOjQ.jpeg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1. Generate right-to-left at certain length (at each stage, select next token candidates with top-K sampling)  \n",
    "\n",
    "2. Take random beam from top-K most probable beams and add to start phrase  \n",
    "\n",
    "3. Generate left-to-right beams with new start phrase\n",
    "\n",
    "4. take random beam from top-K most probable beams and add to phrase, then iterate!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a combination of beam and top-k generation to generate sequences of n tokens from both sides\n",
    "padding_tokens = tokenizer.encode(PADDING_TEXT, add_special_tokens=False)\n",
    "mask_tokens = tokenizer.encode('<mask>', add_special_tokens=False)\n",
    "\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n"
   ]
  },
  {
   "source": [
    "Create function candidates_gen, which takes  \n",
    "- Tokenized start sentence\n",
    "- Sequence of token candidates with probabilities  \n",
    "\n",
    "Then generate **n** probable sequences on right or left side\n",
    "\n",
    "This function will be used iteratively, so that generated token sequences from previous will be input next\n",
    "\n",
    "Returns candidates for mask (ex. input: \"five apples\", output: [for, or, of\\])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_gen(sent_tokens, candidate=([], 1, []), d='left', n_candidates=5, topk=20, temperature=5):\n",
    "    branch_candidates = []  \n",
    "    cand_tokens = candidate[0]\n",
    "    \n",
    "    # First prepare input depending on direction. ex) five apples\n",
    "    if d == 'right':    \n",
    "        input = sent_tokens + cand_tokens + mask_tokens     \n",
    "        \n",
    "        target_id = -1 # mask is at the end, since left-to-right generation\n",
    "        input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)  \n",
    "\n",
    "        perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
    "        perm_mask[0, :, target_id] = 1.0  # Previous tokens don't see last token\n",
    "    else:        \n",
    "        input = mask_tokens + cand_tokens + sent_tokens    \n",
    "        \n",
    "        target_id = -len(input) # mask at the front, since right-to-left generation\n",
    "        input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)  \n",
    "\n",
    "        perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
    "        perm_mask[0, :, [target_id - i for i in range(100)]] = 1.0  # Mask additional previous tokens to improve left-side generation\n",
    "    \n",
    "    # We will predict masked tokens\n",
    "    target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)\n",
    "    target_mapping[0, 0, target_id] = 1.0 # our right prediction\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        input_ids_tensor = input_ids.to(\"cuda\")\n",
    "        target_mapping_tensor = target_mapping.to(\"cuda\")\n",
    "        perm_mask_tensor = perm_mask.to(\"cuda\")\n",
    "    else:\n",
    "        input_ids_tensor = input_ids\n",
    "        target_mapping_tensor = target_mapping\n",
    "        perm_mask_tensor = perm_mask\n",
    "\n",
    "    # Predict mask\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
    "\n",
    "    probs = torch.nn.functional.softmax(outputs[0][0][0]/temperature, dim = 0)\n",
    "    selected_indexes = choose_from_top(probs.to('cpu').numpy(), k=topk, sample_size=n_candidates)\n",
    "    selected_probs = probs[selected_indexes]\n",
    "\n",
    "    # Add possible results to branch_candidates. ex) if right, \"five apples for\", \"fiver apples or\"\n",
    "    for i, item in enumerate(selected_indexes):\n",
    "        the_index = item.item()\n",
    "        if d == 'right':\n",
    "            new_sent = cand_tokens + [the_index]\n",
    "        elif d == \"left\":\n",
    "            new_sent = [the_index] + cand_tokens\n",
    "\n",
    "        prob = selected_probs[i].item()\n",
    "        # add word combinations to branch_candidates in format [sentence, cumulative probability, all probs]\n",
    "        branch_candidates.append((new_sent, candidate[1] * prob, candidate[2] + [prob]))\n",
    "    \n",
    "    return branch_candidates\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "The beam_gen function uses candidates from candidates_gen to form some number of beams (depth below)\n",
    "\n",
    "output: [\"for you\", \"for me\", ...\\]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_gen(sent_tokens, candidates, depth=5, d='right', sample_size=2, topk=10, temperature=5):\n",
    "    beams = candidates[:]\n",
    "    new_candidates = candidates[:]\n",
    "\n",
    "    # create 5 beams\n",
    "    while depth > 0:\n",
    "        new_candidates = []\n",
    "        for candidate in candidates:\n",
    "            for new_candidate in candidates_gen(sent_tokens, candidate, d, sample_size, topk, temperature):\n",
    "                beams.append(new_candidate)\n",
    "                new_candidates.append(new_candidate)\n",
    "        print(\"number of beams:\", len(new_candidates))\n",
    "        candidates = new_candidates[:]\n",
    "        depth -= 1\n",
    "        \n",
    "    # sort candidate beams by a sum of logaryphms of probability of each word in a beam. Which is equivalent to product of probabilities\n",
    "    sorted_beams = sorted(new_candidates, key=lambda tup: np.sum(np.log10(tup[2])), reverse=True)\n",
    "    return beams, sorted_beams"
   ]
  },
  {
   "source": [
    "Finally, the bi_generator function\n",
    "\n",
    "- If direction: both\n",
    "    - generate n_tokens on left side, n_tokens on right side, iterate\n",
    "\n",
    "- first_sample_size: the number of candidates in the first stage of beam search\n",
    "\n",
    "- sample_size: number of candidates in the next stages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_generator(sent, direction, first_sample_size, sample_size, n_tokens, topk, iterations, temperature):\n",
    "    sent_tokens = tokenizer.encode(sent, add_special_tokens=False)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        if (i % 2 == 0 and direction == 'both') or direction == 'left':\n",
    "            print('>> left side generation')\n",
    "            candidates = candidates_gen(sent_tokens=sent_tokens, d='left', n_candidates=first_sample_size, topk=topk, temperature=temperature)\n",
    "            beams, sorted_beams = beam_gen(sent_tokens, candidates, n_tokens-1, 'left', sample_size, temperature=temperature)\n",
    "            topn = len(sorted_beams)//5 if len(sorted_beams) > 4 else len(sorted_beams)\n",
    "            selected_candidate = random.choice(sorted_beams[:topn])\n",
    "            sent_tokens = selected_candidate[0] + sent_tokens\n",
    "            print(tokenizer.decode(sent_tokens))\n",
    "        \n",
    "        if (i % 2 != 0 and direction == 'both') or direction == 'right':\n",
    "            print('>> right side generation')\n",
    "            candidates = candidates_gen(sent_tokens=sent_tokens, d='right', n_candidates=first_sample_size, topk=topk, temperature=temperature)\n",
    "            beams, sorted_beams = beam_gen(sent_tokens, candidates, n_tokens-1, 'right', sample_size, topk, temperature=temperature)\n",
    "            topn = len(sorted_beams)//5 if len(sorted_beams) > 4 else len(sorted_beams)\n",
    "            selected_candidate = random.choice(sorted_beams[:topn])\n",
    "            sent_tokens = sent_tokens + selected_candidate[0]\n",
    "            print(tokenizer.decode(sent_tokens))\n",
    "    return tokenizer.decode(sent_tokens)"
   ]
  },
  {
   "source": [
    "Let's try it out!\n",
    "\n",
    "- Start sentence: text generation is cool\n",
    "\n",
    "- each beam length: 4\n",
    "\n",
    "- iterate 6 times (3 times on each side)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> left side generation\n",
      "number of beams: 8\n",
      "number of beams: 16\n",
      "number of beams: 32\n",
      "creating the environment where text generation is cool\n",
      ">> right side generation\n",
      "number of beams: 8\n",
      "number of beams: 16\n",
      "number of beams: 32\n",
      "creating the environment where text generation is cool is something I have\n",
      ">> left side generation\n",
      "number of beams: 8\n",
      "number of beams: 16\n",
      "number of beams: 32\n",
      "<eod> For me, creating the environment where text generation is cool is something I have\n",
      ">> right side generation\n",
      "number of beams: 8\n",
      "number of beams: 16\n",
      "number of beams: 32\n",
      "<eod> For me, creating the environment where text generation is cool is something I have spent an enormous amount\n",
      ">> left side generation\n",
      "number of beams: 8\n",
      "number of beams: 16\n",
      "number of beams: 32\n",
      "of them.<eop><eod> For me, creating the environment where text generation is cool is something I have spent an enormous amount\n",
      ">> right side generation\n",
      "number of beams: 8\n",
      "number of beams: 16\n",
      "number of beams: 32\n",
      "of them.<eop><eod> For me, creating the environment where text generation is cool is something I have spent an enormous amount of effort over a\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'of them.<eop><eod> For me, creating the environment where text generation is cool is something I have spent an enormous amount of effort over a'"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "sent = \"text generation is cool\"\n",
    "first_sample_size = 4\n",
    "sample_size = 2\n",
    "n_tokens = 4\n",
    "topk = 20\n",
    "iterations = 6\n",
    "temperature = 4\n",
    "direction = \"both\"\n",
    "\n",
    "bi_generator(sent, direction, first_sample_size, sample_size, n_tokens, topk, iterations, temperature)"
   ]
  },
  {
   "source": [
    "# Conclusion\n",
    "We created a transformers based bidirectional text generator!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}